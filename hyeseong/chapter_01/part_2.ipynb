{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파트 2: 고급 기법과 평가\n",
    "\n",
    "### 예제코드 : Chapter01/RAG_Overview.ipynb\n",
    "\n",
    "> part1 환경설정을 그대로 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.path.abspath(os.getcwd())\n",
    "python_path = os.path.join(current_dir, \"..\")\n",
    "\n",
    "if python_path not in sys.path:\n",
    "    sys.path.append(python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cllama import send_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색 지표 (섹션 1) \n",
    "\n",
    "> 텍스트 문서들의 관련성을 평가하는데 중요한 코사인 유사도에 대해 먼저 살펴봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 코사인 유사도는 텍스트의 벡터 표현 간 각도만을 엄격하게 측정하기 때문에 중의적인 쿼리문을 다룰 때는 제한이 있습니다. \n",
    "\n",
    "> 자연어 처리(NLP) 기법들을 활용해서 단어들 사이의 의미 관계를 좀 더 잘 포착하는 계산 방식을 도입한다면 유사도를 좀 더 개선할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeonghyeseong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 1회만 실행: spaCy 모델 다운로드\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 RAG (섹션 2) \n",
    "\n",
    "p.26\n",
    "\n",
    "\n",
    "> 키워드 검색과 매칭을 사용하는 단순 RAG는 법률 문서나 의료 문서와 같이 잘 정의된 조직 내부 문서에서 효율적일 수 있습니다.\n",
    "\n",
    "> `query`, `db_records` 는 앞서 정의한 내용을 그대로 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "query = \"define a rag store\"\n",
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Format the response with textwrap\n",
    "    wrapped_text = textwrap.fill(response, width=80)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 키워드 검색 및 매칭 함수를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 코사인 유사도 지표 함수를 이용해서 쿼리문과 최적 레코드의 유사도 점수를 계산해봅니다.  \n",
    "\n",
    "> 유사도가 높지 않은 이유는 사용자 입력(쿼리문)은 짧은 반면에 응답은 더 길고 완전하기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 개선된 유사도 지표를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사용자 입력과 키워드 검색으로 찾은 데이터셋의 최적 매칭 레코드를 연결해서 입력을 증갑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record\n",
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_with_full_text(itext: str):\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content and tralsate the result : \\n {text_input}\"    \n",
    "    try:\n",
    "        result = send_query([\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert Natural Language Processing excercise expert.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"1. You can explain read the input and answer in detail\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        result = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down this content, elaborate on it, and then provide a more\n",
      "comprehensive explanation, followed by translations into a few common languages.\n",
      "**Understanding the Content: RAG Vector Stores**  The provided text introduces\n",
      "the concept of a \"RAG vector store.\" Let's unpack what that means.  It's a key\n",
      "component in a modern approach to working with information, especially when\n",
      "using large language models (LLMs) like me.  * **RAG stands for Retrieval-\n",
      "Augmented Generation.**  This is the overarching technique. It combines two core\n",
      "ideas:     * **Retrieval:**  Finding relevant information from a knowledge\n",
      "source.     * **Generation:**  Using that retrieved information to generate a\n",
      "response (like a text answer, code snippet, etc.).  The \"generation\" part is\n",
      "typically handled by an LLM.  * **Vector Store:** This is *where* the knowledge\n",
      "for the \"Retrieval\" part is stored.  Here's a detailed explanation of what a\n",
      "vector store is and how it works:      * **Traditional Databases vs. Vector\n",
      "Databases:**  Traditional databases (like MySQL, PostgreSQL) are great at\n",
      "storing structured data (tables, rows, columns). They're efficient at searching\n",
      "for exact matches. However, they struggle with *semantic* search – finding\n",
      "information based on *meaning* rather than keywords.      * **Vectors\n",
      "(Embeddings):**  Vector stores don't store data as text directly. Instead, they\n",
      "store *vector embeddings*.  An embedding is a list of numbers that represents\n",
      "the meaning of a piece of text (or any data, really). These vectors are created\n",
      "using models specifically designed to understand semantics.  Similar meanings\n",
      "will result in vectors that are close to each other in a multi-dimensional\n",
      "space.  Think of it like mapping words into a coordinate system where synonyms\n",
      "are near each other.      * **How it Works:**         1. **Data Ingestion:** You\n",
      "take your knowledge source (documents, articles, web pages, etc.).         2.\n",
      "**Embedding Creation:**  You use an embedding model to convert each piece of\n",
      "data into a vector.         3. **Vector Storage:** These vectors are stored in\n",
      "the vector store.         4. **Query Embedding:**  When a user asks a question,\n",
      "you convert that question into a vector using the *same* embedding model.\n",
      "5. **Similarity Search:** The vector store finds the vectors that are *most\n",
      "similar* to the query vector.  Similarity is typically measured using cosine\n",
      "similarity or other distance metrics.  This finds the most relevant pieces of\n",
      "information.         6. **Context Augmentation:** The retrieved information (the\n",
      "context) is then fed to the LLM along with the user's query.         7.\n",
      "**Generation:**  The LLM uses this combined information (query + context) to\n",
      "generate a more accurate, informed, and contextually relevant answer.  **Why are\n",
      "RAG Vector Stores important?**  * **Access to up-to-date information:** LLMs\n",
      "have a knowledge cutoff date. RAG allows them to access and use information\n",
      "beyond that date. * **Domain-Specific Knowledge:**  You can provide the LLM with\n",
      "knowledge specific to your organization or industry. * **Reduced\n",
      "Hallucinations:** By grounding the LLM in retrieved facts, RAG reduces the\n",
      "chance of it making up information (hallucinations). * **Improved Accuracy:**\n",
      "Contextual information leads to more accurate and relevant responses.    ---\n",
      "**Translations:**  Here are translations of the core definition: \"A RAG vector\n",
      "store is a database or dataset that contains vectorized data points.\"  *\n",
      "**Spanish:** \"Una base de datos vectorial RAG es una base de datos o conjunto de\n",
      "datos que contiene puntos de datos vectorizados.\" * **French:** \"Un stockage\n",
      "vectoriel RAG est une base de données ou un ensemble de données qui contient des\n",
      "points de données vectorisés.\" * **German:** \"Ein RAG-Vektorspeicher ist eine\n",
      "Datenbank oder ein Datensatz, der vektorisierte Datenpunkte enthält.\" *\n",
      "**Chinese (Simplified):** \"RAG向量存储是一种数据库或数据集，包含向量化的数据点。(RAG xiàngliàng cúnchú\n",
      "shì yī zhǒng shùjùkù huò shùjù jí, bāohán xiàngliàng de shùjù diǎn.)\" *\n",
      "**Japanese:** \"RAGベクターストアは、ベクトル化されたデータポイントを含むデータベースまたはデータセットです。(RAG bekutā sutoa\n",
      "wa, bekutōka sareta dēta pointo o fukumu dētabēsu matawa dēta setto desu.)\"\n",
      "Let me know if you'd like a deeper dive into any of these aspects.  For example,\n",
      "we could discuss:  *   Different types of vector databases (Pinecone, Chroma,\n",
      "Weaviate, etc.) *   Embedding models (OpenAI embeddings, Sentence Transformers,\n",
      "etc.) *   How to build a RAG pipeline. *   Advanced RAG techniques (chunking,\n",
      "filtering, re-ranking).\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고급 RAG (섹션 3)\n",
    "\n",
    "> 데이터셋이 커질수록 키워드 검색 방식은 실행 시간이 너무 오래 걸릴 수 있습니다.\n",
    "\n",
    "> 이번 섹션에서는 벡터 검색과 색인 기반 검색을 활용하여 검색 효율성과 처리 속도를 향상하는 방법을 살펴봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터 검색 (섹션 3.1)\n",
    "\n",
    "> 벡터 검색은 사용자 쿼리문과 문서를 벡터라는 수치 배열로 변환해서 수학 계산에 사용합니다. \n",
    "\n",
    "> 덕분에 대용량 데이터를 다룰 때 관련 데이터를 더 빠르게 검색할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n"
     ]
    }
   ],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 단순 RAG 에서도 이 문장이 최적의 레코드로 선택되었었던 것처럼, 단순 RAG가 반드시 고급 RAG 보다 나쁘지는 않음을 말해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\") \n",
    "\n",
    "# Enhanced Similarity\n",
    "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 단순 RAG 와 동일한 레코드가 검색되었으므로 코사인 유사도와 개선된 유사도 모두 동일할 수 밖에 없습니다.\n",
    "\n",
    "> 결과가 같은데 굳이 벡터 검색을 사용할 필요가 있을까 의문이 들지만 데이터셋 규모가 더 커진다면 벡터 검색의 장점이 좀 더 확실히 드러날 겁니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "Okay, let's break down this content. It's a very short and foundational\n",
      "explanation of a **RAG vector store**, a core component in modern Natural\n",
      "Language Processing (NLP) and particularly Retrieval-Augmented Generation (RAG)\n",
      "systems.  Here's a detailed elaboration, followed by translations into several\n",
      "languages:  **Elaboration of \"Define a RAG store: A RAG vector store is a\n",
      "database or dataset that contains vectorized data points.\"**  This statement is\n",
      "defining a key component used in building applications that can \"understand\" and\n",
      "respond to questions based on large amounts of data. Let's unpack it piece by\n",
      "piece:  * **RAG:** Stands for **Retrieval-Augmented Generation**. It's an\n",
      "architecture for building language models (like those powered by LLMs - Large\n",
      "Language Models) that combines the strengths of *information retrieval* with the\n",
      "power of *text generation*.  Think of it as giving the language model access to\n",
      "a knowledge base *before* it answers a question. * **Vector Store:** This is the\n",
      "heart of the 'retrieval' part of RAG.  Traditionally, searching through data\n",
      "meant looking for keyword matches. Vector stores are different.  They store data\n",
      "not as raw text or strings, but as *vector embeddings*.      * **Vector\n",
      "Embeddings:**  These are numerical representations of the *meaning* of your\n",
      "data.  An NLP model (like those from OpenAI, Google, or open-source options)\n",
      "transforms your text (documents, paragraphs, sentences) into a list of numbers.\n",
      "Numbers that are \"close\" to each other in this numerical space represent text\n",
      "that is semantically similar.  This allows for *semantic search* – finding\n",
      "information that's related in *meaning*, even if it doesn't share the same\n",
      "keywords. * **Data Points:**  The things you're storing in the vector store.\n",
      "These could be:     * **Chunks of text:** Documents are often broken down into\n",
      "smaller chunks (paragraphs, sections) to improve retrieval speed and relevance.\n",
      "* **Images:** Vector stores can also store vector embeddings of images.     *\n",
      "**Audio:**  Audio can also be converted into vector embeddings.     * **Any data\n",
      "that can be meaningfully represented as a vector.** * **Database or Dataset:**\n",
      "The vector store is *implemented* as either a specialized database (designed for\n",
      "vectors, like Pinecone, Weaviate, Chroma) or as a dataset that holds the vector\n",
      "embeddings and associated metadata.  **In essence, a RAG vector store is a place\n",
      "to efficiently store and search for information based on *meaning* rather than\n",
      "just keywords. This allows LLMs to access relevant information and generate more\n",
      "accurate, informed, and contextually appropriate responses.**    ---\n",
      "**Translations:**  Here are translations of the original statement and a\n",
      "slightly more expanded version (the original + \"This allows for semantic\n",
      "search\") into several languages:  **Original Statement:** \"A RAG vector store is\n",
      "a database or dataset that contains vectorized data points.\" **Expanded\n",
      "Statement:**  \"A RAG vector store is a database or dataset that contains\n",
      "vectorized data points. This allows for semantic search.\"  * **Spanish\n",
      "(Español):**     * **Original:** \"Un almacén de vectores RAG es una base de\n",
      "datos o conjunto de datos que contiene puntos de datos vectorizados.\"     *\n",
      "**Expanded:** \"Un almacén de vectores RAG es una base de datos o conjunto de\n",
      "datos que contiene puntos de datos vectorizados. Esto permite la búsqueda\n",
      "semántica.\"  * **French (Français):**     * **Original:** \"Un magasin de\n",
      "vecteurs RAG est une base de données ou un ensemble de données qui contient des\n",
      "points de données vectorisés.\"     * **Expanded:** \"Un magasin de vecteurs RAG\n",
      "est une base de données ou un ensemble de données qui contient des points de\n",
      "données vectorisés. Cela permet la recherche sémantique.\"  * **German\n",
      "(Deutsch):**     * **Original:** \"Ein RAG-Vektorspeicher ist eine Datenbank oder\n",
      "ein Datensatz, der vektorisierte Datenpunkte enthält.\"     * **Expanded:** \"Ein\n",
      "RAG-Vektorspeicher ist eine Datenbank oder ein Datensatz, der vektorisierte\n",
      "Datenpunkte enthält. Dies ermöglicht die semantische Suche.\"  * **Chinese\n",
      "(Simplified) (简体中文):**     * **Original:** \"RAG 向量存储是一个数据库或数据集，包含向量化的数据点。\" (RAG\n",
      "xiàngliàng cúnchú shì yīgè shùjùkù huò shùjù jí, bāohán xiàngliàng de shùjù\n",
      "diǎn.)     * **Expanded:** “RAG 向量存储是一个数据库或数据集，包含向量化的数据点。这允许进行语义搜索。”(Zhè yǔnxǔ\n",
      "jìnxíng yǔyì suǒusuǒ.)  * **Japanese (日本語):**     * **Original:**\n",
      "「RAGベクターストアは、ベクトル化されたデータポイントを含むデータベースまたはデータセットです。」(RAG bekutā sutoa wa,\n",
      "bekutorika sareta dēta pointo o fukumu dētabēsu mata wa dēta setto desu.)     *\n",
      "**Expanded:**\n",
      "「RAGベクターストアは、ベクトル化されたデータポイントを含むデータベースまたはデータセットです。これにより、意味検索が可能になります。」(Kore ni\n",
      "yori, imi kensaku ga kanō ni narimasu.)  ---  **Let me know if you'd like:**  *\n",
      "A deeper dive into any specific aspect of RAG or vector stores. *   Translations\n",
      "into other languages. *   Examples of how RAG systems are used. *   Information\n",
      "on specific vector database options.\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+\": \"+best_matching_record\n",
    "\n",
    "print_formatted_response(augmented_input)\n",
    "\n",
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 색인 기반 검색 (섹션 3.2)\n",
    "\n",
    "> 사용자 쿼리문의 벡터를 문서 내용의 직접적인 벡터와 비교하지 않고, 색인화된 벡터(indexed vector)와 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이번에도 최적 레코드가 동일하므로 유사도 역시 이전과 동일합니다. \n",
    "\n",
    "> 다만, 차이는 최적 레코드를 좀 더 빨리 찾아냈다는 점입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "best_cosine_similarity_score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {best_cosine_similarity_score:.3f}\")\n",
    "\n",
    "\n",
    "# Enhanced Similarity\n",
    "enhanced_similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {enhanced_similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 특징 행렬이 어떤 형태인지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
      "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
      "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
      "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "\n",
      "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
      "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
      "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
      "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
      "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
      "\n",
      "       where     which    while     wide      with    within   without  \n",
      "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
      "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
      "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
      "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
      "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
      "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
      "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[28 rows x 297 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(tfidf_df)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 입력을 증강하고 LLM 을 호출해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "Okay, let's break down this content and elaborate on it, then I'll provide a\n",
      "more comprehensive explanation, effectively \"expanding\" on what's given.  I'll\n",
      "aim to explain it in a way that someone unfamiliar with the concept of RAG\n",
      "vector stores can understand, while also hitting on the important technical\n",
      "details.  After the elaboration, I'll provide a translation into a few different\n",
      "languages.  **Elaboration of \"RAG Vector Store\"**  The provided text defines a\n",
      "RAG vector store very simply: it's a database (or dataset) holding vectorized\n",
      "data points. However, that definition needs a lot of context. Here's a more\n",
      "detailed explanation, built up from the core concept:  **1. What is RAG?\n",
      "(Retrieval-Augmented Generation)**  Before diving into the *store* itself, let’s\n",
      "understand the bigger picture: **RAG**. RAG is a technique used with Large\n",
      "Language Models (LLMs) like GPT-3, GPT-4, Gemini, etc.  LLMs are incredibly\n",
      "powerful at *generating* text, but they have limitations:  *   **Knowledge\n",
      "Cutoff:** LLMs are trained on a massive dataset, but that dataset has a point in\n",
      "time. Anything that happened *after* that training date is unknown to the LLM. *\n",
      "**Lack of Specific Knowledge:**  An LLM might not know specific details about\n",
      "*your* company, *your* products, *your* internal documentation, or niche\n",
      "information. *   **Hallucinations:** LLMs can sometimes \"make things up\"\n",
      "(hallucinate) and present them as fact.  RAG addresses these limitations. The\n",
      "core idea is to *retrieve* relevant information from an external source *before*\n",
      "the LLM generates a response.  The LLM then uses this retrieved information to\n",
      "inform its generation, leading to more accurate, relevant, and grounded answers.\n",
      "**2. The Role of the Vector Store**  This is where the \"vector store\" comes in.\n",
      "RAG needs a way to efficiently store and search large amounts of data.\n",
      "Traditional databases aren't ideal for this task, because RAG isn't usually\n",
      "about exact keyword matching. It's about *semantic* similarity – finding\n",
      "information that *means* the same thing, even if the words are different.\n",
      "Here's how it works:  *   **Data Preparation:** You start with your data\n",
      "(documents, text, web pages, etc.). *   **Embedding (Vectorization):**  This is\n",
      "the crucial step. An **embedding model** (a type of neural network) converts\n",
      "each piece of text into a **vector** - a list of numbers.  This vector\n",
      "represents the *meaning* of the text.  Similar pieces of text will have similar\n",
      "vectors (vectors that are close to each other in multi-dimensional space).  This\n",
      "process is also known as \"vectorization\". *   **Storing Vectors:** These vectors\n",
      "are stored in the **vector store**.  Unlike a traditional database, a vector\n",
      "store is optimized for fast similarity searches. *   **Retrieval:** When a user\n",
      "asks a question, that question is *also* converted into a vector using the same\n",
      "embedding model.  The vector store then finds the vectors that are most\n",
      "*similar* to the question vector. *   **Augmentation & Generation:** The\n",
      "corresponding text associated with those similar vectors is retrieved and\n",
      "provided to the LLM. The LLM then uses this retrieved context to generate its\n",
      "response.    **3. Types of Vector Stores**  There are many different types of\n",
      "vector stores available. Some popular ones include:  *   **Pinecone:** A fully\n",
      "managed vector database service. *   **Weaviate:** An open-source, graph-based\n",
      "vector database. *   **Chroma:**  An open-source embedding database. *   **FAISS\n",
      "(Facebook AI Similarity Search):** A library for efficient similarity search.\n",
      "Often used in conjunction with other databases. *   **Milvus:** Another open-\n",
      "source vector database.  **4. Why use Vector Stores?**  *   **Semantic Search:**\n",
      "Find information based on meaning, not just keywords. *   **Scalability:**\n",
      "Handle large volumes of data efficiently. *   **Speed:** Perform similarity\n",
      "searches quickly, even with millions of vectors. *   **Integration with LLMs:**\n",
      "Designed to work seamlessly with LLMs for RAG applications.     **In summary:**\n",
      "A RAG vector store is a specialized database that stores vector representations\n",
      "of data, enabling efficient semantic search and retrieval to augment the\n",
      "knowledge of Large Language Models.    ---  **Translations**  Here are\n",
      "translations of the *elaborated* explanation (a summary of the above) into a few\n",
      "languages. It's difficult to provide a *perfect* translation without knowing the\n",
      "target audience's level of technical expertise, but I'll aim for clarity.  I'll\n",
      "provide a shortened version for brevity.  **Spanish (Español):**  \"Una base de\n",
      "datos vectorial RAG (Retrieval-Augmented Generation) es una base de datos\n",
      "especializada que almacena representaciones vectoriales de datos.  Se utiliza\n",
      "para mejorar la precisión y relevancia de los modelos de lenguaje grandes (LLM)\n",
      "al permitir la búsqueda semántica eficiente. En lugar de buscar coincidencias de\n",
      "palabras clave, busca información que tenga un significado similar a la pregunta\n",
      "del usuario. Esto permite a los LLM responder preguntas con información más\n",
      "precisa y actualizada, incluso si esa información no estaba presente en sus\n",
      "datos de entrenamiento originales.\"  **French (Français):**  \"Un magasin\n",
      "vectoriel RAG (Retrieval-Augmented Generation) est une base de données\n",
      "spécialisée qui stocke des représentations vectorielles de données. Il est\n",
      "utilisé pour améliorer la précision et la pertinence des grands modèles de\n",
      "langage (LLM) en permettant une recherche sémantique efficace. Au lieu de\n",
      "rechercher des correspondances de mots-clés, il recherche des informations qui\n",
      "ont une signification similaire à la question de l'utilisateur. Cela permet aux\n",
      "LLM de répondre aux questions avec des informations plus précises et à jour,\n",
      "même si ces informations n'étaient pas présentes dans leurs données\n",
      "d'entraînement originales.\"  **German (Deutsch):**  \"Ein RAG-Vektor-Store\n",
      "(Retrieval-Augmented Generation) ist eine spezialisierte Datenbank, die\n",
      "vektorielle Darstellungen von Daten speichert. Er wird verwendet, um die\n",
      "Genauigkeit und Relevanz großer Sprachmodelle (LLMs) zu verbessern, indem eine\n",
      "effiziente semantische Suche ermöglicht wird. Anstatt nach\n",
      "Schlüsselwortübereinstimmungen zu suchen, sucht er nach Informationen, die eine\n",
      "ähnliche Bedeutung wie die Frage des Benutzers haben. Dadurch können LLMs Fragen\n",
      "mit präziseren und aktuelleren Informationen beantworten, auch wenn diese\n",
      "Informationen nicht in ihren ursprünglichen Trainingsdaten enthalten waren.\"\n",
      "**Simplified Chinese (简体中文):**  “RAG\n",
      "向量存储（检索增强生成）是一种专门的数据库，用于存储数据的向量表示。它通过实现高效的语义搜索，提高大型语言模型 (LLM)\n",
      "的准确性和相关性。它不是搜索关键词匹配，而是搜索具有与用户查询相似含义的信息。这使得 LLM\n",
      "能够使用更准确和最新的信息回答问题，即使这些信息不在其原始训练数据中。”    Let me know if you'd like me to:  *\n",
      "Elaborate further on any specific aspect of RAG or vector stores. *   Provide\n",
      "translations into other languages. *   Explain the code implementation of a\n",
      "basic RAG system. *   Discuss advanced topics like vector database indexing or\n",
      "embedding model selection.\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+\": \"+best_matching_record\n",
    "print_formatted_response(augmented_input)\n",
    "\n",
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모듈형 RAG (섹션 4)\n",
    "\n",
    "* `키워드 검색`은 단순 검색에 적합\n",
    "* `벡터 검색`은 의미론적으로 풍부한 문서에 이상적\n",
    "* `색인 기반 검색`은 대규모 데이터에서 빠른 속도를 제공\n",
    "\n",
    "> 각 접근 방식에는 장점이 있습니다. 중요한 것은, 세 방법을 하나의 프로젝트 안에서 조화롭게 사용할 수 있다는 점입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "      self.documents = records  # Initialize self.documents here\n",
    "      if self.method == 'vector' or self.method == 'indexed':\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
    "retrieval.fit(db_records)\n",
    "best_matching_record = retrieval.retrieve(query)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "Okay, let's break down this content. It's a very concise definition of a **RAG\n",
      "vector store** within the context of Natural Language Processing (NLP) and\n",
      "specifically, Retrieval Augmented Generation (RAG).  I'll elaborate on it,\n",
      "explaining each component, its purpose, and the broader context. Then I'll\n",
      "provide translations into several languages.  **Elaboration of the Content:**\n",
      "The text defines a RAG vector store. Let's dissect that:  * **RAG (Retrieval\n",
      "Augmented Generation):** This is a key architectural pattern in modern NLP.\n",
      "Instead of a large language model (LLM) *only* relying on its pre-trained\n",
      "knowledge, RAG enhances it by *retrieving* relevant information from an external\n",
      "knowledge source *before* generating a response.  This allows the LLM to provide\n",
      "more accurate, up-to-date, and contextually relevant answers. Think of it as\n",
      "giving the LLM \"cheat sheets\" to consult before it answers.  * **Vector Store:**\n",
      "This is the *how* RAG retrieves the information.  It's the database/dataset\n",
      "where the knowledge is stored, but *not* in a traditional format (like rows and\n",
      "columns in a relational database).  Instead, it stores data as **vectors**.\n",
      "Let's dive deeper into that:      * **Vectorization:** The process of turning\n",
      "text (or other data like images, audio, etc.) into numerical representations\n",
      "called vectors. These vectors capture the *semantic meaning* of the data.\n",
      "Similar pieces of text will have vectors that are close to each other in a\n",
      "multi-dimensional space.  This is done using *embedding models* (like those from\n",
      "OpenAI, Google, or open-source models).      * **Why Vectors?** Vectors allow\n",
      "for *semantic search*.  Instead of searching for keywords, you can search for\n",
      "*meaning*.  You can ask a question (which is also vectorized) and find the\n",
      "vectors in the store that are most similar.  This is done using techniques like\n",
      "cosine similarity.      * **Database/Dataset:**  The vector store isn't just a\n",
      "list of vectors.  It's usually a specialized database designed to efficiently\n",
      "store and search through a large number of high-dimensional vectors.  Popular\n",
      "options include:         * **Pinecone:** A fully managed vector database.\n",
      "* **Chroma:** An open-source embedding database.         * **Weaviate:**\n",
      "Another open-source vector database with more graph-like capabilities.         *\n",
      "**FAISS (Facebook AI Similarity Search):**  A library for efficient similarity\n",
      "search.         * **Milvus:**  Another open-source vector database.         *\n",
      "**Even traditional databases (like PostgreSQL with the `pgvector` extension) can\n",
      "be used.**  * **Vectorized Data Points:** These are the individual pieces of\n",
      "information stored in the vector store. They could be:     * **Chunks of text:**\n",
      "Documents are typically broken down into smaller chunks for more efficient\n",
      "retrieval.     * **Images (represented as vectors).**     * **Audio clips\n",
      "(represented as vectors).**     * **Any other type of data that can be\n",
      "meaningfully vectorized.**    **In summary:**  A RAG vector store is a\n",
      "specialized database that stores vector representations of data, enabling\n",
      "efficient semantic search for use in Retrieval Augmented Generation systems.\n",
      "It's a crucial component for building LLM-powered applications that require\n",
      "access to current, relevant, and specific information.    ---  **Translations:**\n",
      "Here are translations of the definition into several languages:  **1. Spanish\n",
      "(Español):**  \"Un almacén de vectores RAG (Generación Aumentada por\n",
      "Recuperación) es una base de datos o conjunto de datos que contiene puntos de\n",
      "datos vectorizados.  Se utiliza para almacenar representaciones numéricas\n",
      "(vectores) de datos, lo que permite una búsqueda semántica eficiente para\n",
      "sistemas de RAG, permitiendo a los modelos de lenguaje acceder a información\n",
      "relevante y actualizada.\"  **2. French (Français):**  \"Un magasin de vecteurs\n",
      "RAG (Génération Augmentée par Récupération) est une base de données ou un\n",
      "ensemble de données qui contient des points de données vectorisés. Il est\n",
      "utilisé pour stocker des représentations numériques (vecteurs) de données,\n",
      "permettant une recherche sémantique efficace pour les systèmes RAG, ce qui\n",
      "permet aux modèles linguistiques d'accéder à des informations pertinentes et à\n",
      "jour.\"  **3. German (Deutsch):**  \"Ein RAG-Vektor-Store (Retrieval Augmented\n",
      "Generation) ist eine Datenbank oder ein Datensatz, der vektorisierte Datenpunkte\n",
      "enthält. Er wird verwendet, um numerische Repräsentationen (Vektoren) von Daten\n",
      "zu speichern, was eine effiziente semantische Suche für RAG-Systeme ermöglicht,\n",
      "sodass Sprachmodelle auf relevante und aktuelle Informationen zugreifen können.\"\n",
      "**4. Chinese (Simplified) (简体中文):**  \"RAG 向量存储 (检索增强生成)\n",
      "是一个数据库或数据集，包含向量化的数据点。它用于存储数据的数值表示（向量），以便为 RAG 系统提供高效的语义搜索，使语言模型能够访问相关和最新的信息。\"\n",
      "(RAG liàngbān chúcún shì yī gè shùjùkù huò shùjù jí, bāohán véngliàng huà de\n",
      "shùjù diǎn. Tā yòng yú chúcún shùjù de shùzhí biǎoshì (véngliàng), yǐbiàn wèi\n",
      "RAG xìtǒng tígōng gāoxiào de yǔyì suǒsuǒ, shǐ yǔyán móxíng nénggòu huòjí\n",
      "xiāngguān hé zuìxīn de xìnxī.)  **5. Japanese (日本語):**  \"RAGベクターストア（検索拡張生成）は、ベクト\n",
      "ル化されたデータポイントを含むデータベースまたはデータセットです。これは、RAGシステムが関連性の高い最新の情報をアクセスできるように、データの数値表現（ベクト\n",
      "ル）を保存するために使用されます。\" (RAG bekutā sutoa wa, bekutorka sareta dēta pointo o fukumu\n",
      "dētabēsu matawa dēta setto desu. Kore wa, RAG shisutemu ga kanren-sei no takai\n",
      "saishin no jōhō o akusesu dekiru yō ni, dēta no sūchi hyōgen (bekutā) o hozon\n",
      "suru tame ni shiyō saremasu.)  ---  I hope this detailed explanation and the\n",
      "translations are helpful! Do you have any other content you'd like me to\n",
      "elaborate on, or would you like me to dive deeper into any specific aspect of\n",
      "RAG or vector stores?  Perhaps you'd like to explore different vector database\n",
      "options, or discuss the process of creating vector embeddings?\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+\": \"+best_matching_record\n",
    "print_formatted_response(augmented_input)\n",
    "\n",
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
