{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:$(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cllama import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/rangkim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chapter 1의 데이터 및 프린터 함수\n",
    "import textwrap\n",
    "query = \"define a rag store\"\n",
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Format the response with textwrap\n",
    "    wrapped_text = textwrap.fill(response, width=80)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n"
     ]
    }
   ],
   "source": [
    "## 26페이지 : 단순 RAG(섹션 2)\n",
    "\n",
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record\n",
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_with_full_text(itext: str):\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content and tralsate the result : \\n {text_input}\"    \n",
    "    try:\n",
    "        result = send_query([\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert Natural Language Processing excercise expert.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"1. You can explain read the input and answer in detail\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        result = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Okay, let's break down this content. It's a very basic definition of a **RAG vector store**.  I'll elaborate on it, explaining what each part means in the context of Natural Language Processing (NLP) and then provide translations into a few common languages.\n",
      "\n",
      "**Elaboration:**\n",
      "\n",
      "The provided text defines a RAG vector store. Let's unpack that phrase by phrase:\n",
      "\n",
      "* **RAG:** Stands for **Retrieval-Augmented Generation**.  This is a technique used in NLP to improve the performance of Large Language Models (LLMs) like GPT-3, GPT-4, Gemini, etc.  The idea is that instead of *just* relying on the LLM's pre-existing knowledge (which can be outdated or incomplete), you *retrieve* relevant information from an external source *before* the LLM generates a response.  This \"augmentation\" makes the LLM's responses more accurate, grounded in facts, and specific to the user's query.\n",
      "\n",
      "* **Vector Store:** This is the *key* component.  It's a specialized type of database designed to store data as *vectors*.  But why vectors?\n",
      "\n",
      "    * **Vectors & Embeddings:**  In NLP, we convert text (words, sentences, documents) into numerical representations called *embeddings*. These embeddings capture the semantic meaning of the text. Similar text will have similar vectors (close proximity in vector space). Think of it like a map where words with related meanings are located near each other.  Tools like OpenAI's embeddings API, Sentence Transformers, or Cohere's embedding models are used to generate these vectors.\n",
      "\n",
      "    * **Storing & Searching:**  A vector store efficiently stores these embeddings.  Critically, it allows for *similarity search*.  Instead of searching for exact keyword matches, you can search for vectors that are *closest* to a given query vector.  This is how RAG finds the most relevant information.\n",
      "\n",
      "* **Data Points:**  These are the pieces of information that are being stored in the vector store.  They could be:\n",
      "    * **Chunks of text:**  Documents are often split into smaller chunks to improve search relevance.\n",
      "    * **Sentences:** Individual sentences can be stored as data points.\n",
      "    * **Other data:**  It's not *just* text.  You can also store embeddings of images, audio, or other data types.\n",
      "\n",
      "**In essence, a RAG vector store is a database of meaningful numerical representations of data that allows for quick and efficient retrieval of information based on semantic similarity.**\n",
      "\n",
      "**How it works in a RAG pipeline:**\n",
      "\n",
      "1. **User Query:** The user asks a question.\n",
      "2. **Query Embedding:** The query is converted into a vector embedding.\n",
      "3. **Similarity Search:** The vector store is searched for vectors that are most similar to the query vector.\n",
      "4. **Retrieval:** The corresponding data points (text chunks, sentences, etc.) are retrieved.\n",
      "5. **Augmentation:** The retrieved data is combined with the original query.\n",
      "6. **Generation:** The LLM uses this augmented prompt to generate a more informed and accurate response.\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "**Translations:**\n",
      "\n",
      "Here are translations of the definition into Spanish, French, and German.  I'll aim for accuracy and clarity.\n",
      "\n",
      "**Spanish (Español):**\n",
      "\n",
      "\"Un almacén de vectores RAG (Retrieval-Augmented Generation) es una base de datos o conjunto de datos que contiene puntos de datos vectorizados.  En el contexto del Procesamiento del Lenguaje Natural (PNL), se utiliza para almacenar representaciones numéricas (vectores) del texto, capturando su significado semántico.  Esto permite la búsqueda de información relevante basada en la similitud del significado, en lugar de simples coincidencias de palabras clave.  Los almacenes de vectores RAG son un componente clave en sistemas que utilizan la técnica de RAG para mejorar la precisión y la calidad de las respuestas generadas por modelos de lenguaje grandes (LLMs).\"\n",
      "\n",
      "**French (Français):**\n",
      "\n",
      "\"Un magasin de vecteurs RAG (Retrieval-Augmented Generation) est une base de données ou un ensemble de données qui contient des points de données vectorisés.  Dans le contexte du Traitement Automatique du Langage Naturel (TALN), il est utilisé pour stocker des représentations numériques (vecteurs) du texte, capturant sa signification sémantique. Cela permet la recherche d'informations pertinentes basée sur la similarité du sens, plutôt que sur de simples correspondances de mots-clés. Les magasins de vecteurs RAG sont un composant clé des systèmes qui utilisent la technique RAG pour améliorer la précision et la qualité des réponses générées par les grands modèles de langage (LLMs).\"\n",
      "\n",
      "**German (Deutsch):**\n",
      "\n",
      "\"Ein RAG-Vektorspeicher (Retrieval-Augmented Generation) ist eine Datenbank oder ein Datensatz, der vektorisierte Datenpunkte enthält. Im Kontext der Verarbeitung natürlicher Sprache (NLP) wird er verwendet, um numerische Repräsentationen (Vektoren) von Text zu speichern, die dessen semantische Bedeutung erfassen. Dies ermöglicht die Suche nach relevanten Informationen basierend auf der semantischen Ähnlichkeit, anstatt auf einfachen Schlüsselwortübereinstimmungen. RAG-Vektorspeicher sind eine Schlüsselkomponente in Systemen, die die RAG-Technik verwenden, um die Genauigkeit und Qualität der von großen Sprachmodellen (LLMs) generierten Antworten zu verbessern.\"\n",
      "\n",
      "\n",
      "\n",
      "Is there anything specific about RAG vector stores you'd like me to elaborate on further? For example, we could discuss:\n",
      "\n",
      "*   Different types of vector databases (e.g., Pinecone, Chroma, FAISS)\n",
      "*   Embedding models and choosing the right one\n",
      "*   Chunking strategies for text data\n",
      "*   How to build a RAG pipeline using Python libraries.\n",
      "Okay, let's break down this content. It's a very basic definition of a **RAG\n",
      "vector store**.  I'll elaborate on it, explaining what each part means in the\n",
      "context of Natural Language Processing (NLP) and then provide translations into\n",
      "a few common languages.  **Elaboration:**  The provided text defines a RAG\n",
      "vector store. Let's unpack that phrase by phrase:  * **RAG:** Stands for\n",
      "**Retrieval-Augmented Generation**.  This is a technique used in NLP to improve\n",
      "the performance of Large Language Models (LLMs) like GPT-3, GPT-4, Gemini, etc.\n",
      "The idea is that instead of *just* relying on the LLM's pre-existing knowledge\n",
      "(which can be outdated or incomplete), you *retrieve* relevant information from\n",
      "an external source *before* the LLM generates a response.  This \"augmentation\"\n",
      "makes the LLM's responses more accurate, grounded in facts, and specific to the\n",
      "user's query.  * **Vector Store:** This is the *key* component.  It's a\n",
      "specialized type of database designed to store data as *vectors*.  But why\n",
      "vectors?      * **Vectors & Embeddings:**  In NLP, we convert text (words,\n",
      "sentences, documents) into numerical representations called *embeddings*. These\n",
      "embeddings capture the semantic meaning of the text. Similar text will have\n",
      "similar vectors (close proximity in vector space). Think of it like a map where\n",
      "words with related meanings are located near each other.  Tools like OpenAI's\n",
      "embeddings API, Sentence Transformers, or Cohere's embedding models are used to\n",
      "generate these vectors.      * **Storing & Searching:**  A vector store\n",
      "efficiently stores these embeddings.  Critically, it allows for *similarity\n",
      "search*.  Instead of searching for exact keyword matches, you can search for\n",
      "vectors that are *closest* to a given query vector.  This is how RAG finds the\n",
      "most relevant information.  * **Data Points:**  These are the pieces of\n",
      "information that are being stored in the vector store.  They could be:     *\n",
      "**Chunks of text:**  Documents are often split into smaller chunks to improve\n",
      "search relevance.     * **Sentences:** Individual sentences can be stored as\n",
      "data points.     * **Other data:**  It's not *just* text.  You can also store\n",
      "embeddings of images, audio, or other data types.  **In essence, a RAG vector\n",
      "store is a database of meaningful numerical representations of data that allows\n",
      "for quick and efficient retrieval of information based on semantic similarity.**\n",
      "**How it works in a RAG pipeline:**  1. **User Query:** The user asks a\n",
      "question. 2. **Query Embedding:** The query is converted into a vector\n",
      "embedding. 3. **Similarity Search:** The vector store is searched for vectors\n",
      "that are most similar to the query vector. 4. **Retrieval:** The corresponding\n",
      "data points (text chunks, sentences, etc.) are retrieved. 5. **Augmentation:**\n",
      "The retrieved data is combined with the original query. 6. **Generation:** The\n",
      "LLM uses this augmented prompt to generate a more informed and accurate\n",
      "response.    ---  **Translations:**  Here are translations of the definition\n",
      "into Spanish, French, and German.  I'll aim for accuracy and clarity.  **Spanish\n",
      "(Español):**  \"Un almacén de vectores RAG (Retrieval-Augmented Generation) es\n",
      "una base de datos o conjunto de datos que contiene puntos de datos vectorizados.\n",
      "En el contexto del Procesamiento del Lenguaje Natural (PNL), se utiliza para\n",
      "almacenar representaciones numéricas (vectores) del texto, capturando su\n",
      "significado semántico.  Esto permite la búsqueda de información relevante basada\n",
      "en la similitud del significado, en lugar de simples coincidencias de palabras\n",
      "clave.  Los almacenes de vectores RAG son un componente clave en sistemas que\n",
      "utilizan la técnica de RAG para mejorar la precisión y la calidad de las\n",
      "respuestas generadas por modelos de lenguaje grandes (LLMs).\"  **French\n",
      "(Français):**  \"Un magasin de vecteurs RAG (Retrieval-Augmented Generation) est\n",
      "une base de données ou un ensemble de données qui contient des points de données\n",
      "vectorisés.  Dans le contexte du Traitement Automatique du Langage Naturel\n",
      "(TALN), il est utilisé pour stocker des représentations numériques (vecteurs) du\n",
      "texte, capturant sa signification sémantique. Cela permet la recherche\n",
      "d'informations pertinentes basée sur la similarité du sens, plutôt que sur de\n",
      "simples correspondances de mots-clés. Les magasins de vecteurs RAG sont un\n",
      "composant clé des systèmes qui utilisent la technique RAG pour améliorer la\n",
      "précision et la qualité des réponses générées par les grands modèles de langage\n",
      "(LLMs).\"  **German (Deutsch):**  \"Ein RAG-Vektorspeicher (Retrieval-Augmented\n",
      "Generation) ist eine Datenbank oder ein Datensatz, der vektorisierte Datenpunkte\n",
      "enthält. Im Kontext der Verarbeitung natürlicher Sprache (NLP) wird er\n",
      "verwendet, um numerische Repräsentationen (Vektoren) von Text zu speichern, die\n",
      "dessen semantische Bedeutung erfassen. Dies ermöglicht die Suche nach relevanten\n",
      "Informationen basierend auf der semantischen Ähnlichkeit, anstatt auf einfachen\n",
      "Schlüsselwortübereinstimmungen. RAG-Vektorspeicher sind eine Schlüsselkomponente\n",
      "in Systemen, die die RAG-Technik verwenden, um die Genauigkeit und Qualität der\n",
      "von großen Sprachmodellen (LLMs) generierten Antworten zu verbessern.\"    Is\n",
      "there anything specific about RAG vector stores you'd like me to elaborate on\n",
      "further? For example, we could discuss:  *   Different types of vector databases\n",
      "(e.g., Pinecone, Chroma, FAISS) *   Embedding models and choosing the right one\n",
      "*   Chunking strategies for text data *   How to build a RAG pipeline using\n",
      "Python libraries.\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
